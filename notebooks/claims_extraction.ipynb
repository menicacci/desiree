{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from scripts import download\n",
    "from scripts.constants import Constants\n",
    "from scripts.table.table_constants import TableConstants\n",
    "from scripts.table.table_prompt import TablePrompts\n",
    "from scripts.llm.llm_request import ParallelAPIRequester\n",
    "from scripts.evaluation import stats\n",
    "from scripts.table import table_extraction, table_types, table_stats\n",
    "from scripts.llm import llm_stats\n",
    "\n",
    "project_path = os.path.abspath(os.path.expandvars(Constants.PROJECT_PATH))\n",
    "\n",
    "experiments_path = os.path.join(project_path, Constants.Directories.EXPERIMENTS)\n",
    "\n",
    "extracted_tables_path = os.path.join(experiments_path, Constants.Directories.EXTRACTED_TABLE)\n",
    "config_path = os.path.join(project_path, Constants.Directories.CONFIGURATIONS)\n",
    "comparison_path = os.path.join(experiments_path, Constants.Directories.COMPARISONS)\n",
    "ground_truth_path = os.path.join(experiments_path, Constants.Directories.GROUND_TRUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dir = \"CS_Test_Small\"\n",
    "articles_path = os.path.join(experiments_path, Constants.Directories.ARTICLES, article_dir)\n",
    "\n",
    "json_file_name = f\"{article_dir}.json\"\n",
    "extracted_tables_file_path = os.path.join(extracted_tables_path, json_file_name)\n",
    "\n",
    "# Download articles as HTML files\n",
    "# search_query = '\"Entity Matching\"'\n",
    "# download.get_articles(search_query, articles_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_extraction.extract_and_save_tables(articles_path, extracted_tables_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_extraction.check_extracted_data(os.path.join(extracted_tables_path, json_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claim Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"cs_test_small_standard_prompt\"\n",
    "tables_file_name = json_file_name\n",
    "msgs_dir = \"CS_Standard\"\n",
    "\n",
    "request_path = os.path.join(experiments_path, Constants.Directories.OUTPUT, output_dir)\n",
    "msgs_path = os.path.join(project_path, Constants.Directories.MESSAGES, msgs_dir)\n",
    "\n",
    "connection_info_path = os.path.join(config_path, \"connection_info.json\")\n",
    "request_conf_path = os.path.join(\n",
    "    config_path, Constants.Directories.API, \"standard_parallel.json\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_prompts = TablePrompts(request_path, extracted_tables_file_path, msgs_path)\n",
    "prompts = table_prompts.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ParallelAPIRequester(\n",
    "    connection_info_path,\n",
    "    request_conf_path\n",
    ")\n",
    "\n",
    "results = llm.run(\n",
    "    request_path,\n",
    "    prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_answer_path = os.path.join(ground_truth_path, \"claim_labels\")\n",
    "\n",
    "# gt_file = os.path.join(ground_truth_path, \"gt_labeled.ods\")\n",
    "# stats.write_ground_truth(gt_file, gt_answer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dir_name = \"claim_types_standard_prompt\"\n",
    "comparison_result_path = os.path.join(comparison_path, comp_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_types.check_claims_types(gt_answer_path, request_path, comparison_result_path)\n",
    "\n",
    "# request_paths = [\n",
    "#     request_path\n",
    "# ]\n",
    "\n",
    "# output_types = TableConstants.Types.CLAIMED_TYPES\n",
    "\n",
    "# table_stats.compare_multiple_results(\n",
    "#     ground_truth_path=gt_answer_path,\n",
    "#     request_paths=request_paths,\n",
    "#     comparison_path=comparison_result_path\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
